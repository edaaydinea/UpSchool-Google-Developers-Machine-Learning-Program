{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C3/W2/ungraded_labs/C3_W2_Lab_3_imdb_subwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLKIel77CJPi"
   },
   "source": [
    "## Ungraded Lab: Subword Tokenization with the IMDB Reviews Dataset\n",
    "\n",
    "In this lab, you will look at a pre-tokenized dataset that is using subword text encoding. This is an alternative to word-based tokenization which you have been using in the previous labs. You will see how it works and its implications on preparing your data and training your model.\n",
    "\n",
    "Let's begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrzOn9quZ0Sv"
   },
   "source": [
    "## Download the IMDB reviews plain text and tokenized datasets\n",
    "\n",
    "First, you will download the [IMDB Reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews) dataset from Tensorflow Datasets. You will get two configurations:\n",
    "\n",
    "* `plain_text` - this is the default and the one you used in Lab 1 of this week\n",
    "* `subwords8k` - a pre-tokenized dataset (i.e. instead of sentences of type string, it will already give you the tokenized sequences). You will see how this looks in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_IoM4VFxWpMR"
   },
   "outputs": [
    {
     "ename": "DatasetNotFoundError",
     "evalue": "Dataset imdb_reviews not found.\nAvailable datasets:\n\t- abstract_reasoning\n\t- accentdb\n\t- aeslc\n\t- aflw2k3d\n\t- ag_news_subset\n\t- ai2_arc\n\t- ai2_arc_with_ir\n\t- amazon_us_reviews\n\t- anli\n\t- answer_equivalence\n\t- arc\n\t- asqa\n\t- asset\n\t- assin2\n\t- bair_robot_pushing_small\n\t- bccd\n\t- beans\n\t- bee_dataset\n\t- beir\n\t- big_patent\n\t- bigearthnet\n\t- billsum\n\t- binarized_mnist\n\t- binary_alpha_digits\n\t- ble_wind_field\n\t- blimp\n\t- booksum\n\t- bool_q\n\t- bucc\n\t- caltech101\n\t- caltech_birds2010\n\t- caltech_birds2011\n\t- cardiotox\n\t- cars196\n\t- cassava\n\t- cats_vs_dogs\n\t- celeb_a\n\t- celeb_a_hq\n\t- chexpert\n\t- cifar10\n\t- cifar100\n\t- cifar100_n\n\t- cifar10_1\n\t- cifar10_corrupted\n\t- cifar10_n\n\t- citrus_leaves\n\t- cityscapes\n\t- clevr\n\t- clic\n\t- cmaterdb\n\t- coco\n\t- coco_captions\n\t- coil100\n\t- colorectal_histology\n\t- colorectal_histology_large\n\t- common_voice\n\t- controlled_noisy_web_labels\n\t- crema_d\n\t- curated_breast_imaging_ddsm\n\t- cycle_gan\n\t- deep_weeds\n\t- dementiabank\n\t- diabetic_retinopathy_detection\n\t- div2k\n\t- dmlab\n\t- domainnet\n\t- downsampled_imagenet\n\t- dsprites\n\t- dtd\n\t- duke_ultrasound\n\t- e2e_cleaned\n\t- efron_morris75\n\t- emnist\n\t- eurosat\n\t- fashion_mnist\n\t- flic\n\t- food101\n\t- fuss\n\t- geirhos_conflict_stimuli\n\t- groove\n\t- gtzan\n\t- gtzan_music_speech\n\t- horses_or_humans\n\t- i_naturalist2017\n\t- i_naturalist2018\n\t- i_naturalist2021\n\t- imagenet2012\n\t- imagenet2012_corrupted\n\t- imagenet2012_fewshot\n\t- imagenet2012_multilabel\n\t- imagenet2012_real\n\t- imagenet2012_subset\n\t- imagenet_a\n\t- imagenet_lt\n\t- imagenet_r\n\t- imagenet_resized\n\t- imagenet_sketch\n\t- imagenet_v2\n\t- imagenette\n\t- imagewang\n\t- kitti\n\t- kmnist\n\t- lfw\n\t- librispeech\n\t- libritts\n\t- ljspeech\n\t- lost_and_found\n\t- lsun\n\t- lvis\n\t- malaria\n\t- mnist\n\t- mnist_corrupted\n\t- nsynth\n\t- nyu_depth_v2\n\t- ogbg_molpcba\n\t- omniglot\n\t- open_images_challenge2019_detection\n\t- open_images_v4\n\t- oxford_flowers102\n\t- oxford_iiit_pet\n\t- pass\n\t- patch_camelyon\n\t- pet_finder\n\t- places365_small\n\t- placesfull\n\t- plant_leaves\n\t- plant_village\n\t- plantae_k\n\t- quickdraw_bitmap\n\t- resisc45\n\t- rock_paper_scissors\n\t- s3o4d\n\t- savee\n\t- scene_parse150\n\t- shapes3d\n\t- siscore\n\t- smallnorb\n\t- so2sat\n\t- speech_commands\n\t- spoken_digit\n\t- stanford_dogs\n\t- stanford_online_products\n\t- stl10\n\t- sun397\n\t- svhn_cropped\n\t- symmetric_solids\n\t- tedlium\n\t- tf_flowers\n\t- the300w_lp\n\t- uc_merced\n\t- universal_dependencies\n\t- user_libri_audio\n\t- vctk\n\t- visual_domain_decathlon\n\t- voc\n\t- voxceleb\n\t- voxforge\n\t- waymo_open_dataset\n\t- wider_face\n\t- xtreme_pos\n\t- xtreme_s\n\t- yes_no\n\nCheck that:\n    - if dataset was added recently, it may only be available\n      in `tfds-nightly`\n    - the dataset name is spelled correctly\n    - dataset class defines all base class abstract methods\n    - the module defining the dataset class is imported\n\nDid you mean: imdb_reviews -> amazon_us_reviews ?\n\nThe builder directory C:\\Users\\Eda AYDIN\\tensorflow_datasets\\imdb_reviews doesn't contain any versions.\nNo builder could be found in the directory: C:\\Users\\Eda AYDIN\\tensorflow_datasets for the builder: imdb_reviews.\nNo registered data_dirs were found in:\n\t- C:\\Users\\Eda AYDIN\\tensorflow_datasets\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mDatasetNotFoundError\u001B[0m                      Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtfds\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Download the plain text default config\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m imdb_plaintext, info_plaintext \u001B[38;5;241m=\u001B[39m \u001B[43mtfds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimdb_reviews\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwith_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mas_supervised\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Download the subword encoded pretokenized dataset\u001B[39;00m\n\u001B[0;32m      7\u001B[0m imdb_subwords, info_subwords \u001B[38;5;241m=\u001B[39m tfds\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimdb_reviews/subwords8k\u001B[39m\u001B[38;5;124m\"\u001B[39m, with_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, as_supervised\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:273\u001B[0m, in \u001B[0;36mload.<locals>.decorator\u001B[1;34m(function, unused_none_instance, args, kwargs)\u001B[0m\n\u001B[0;32m    271\u001B[0m name \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m args \u001B[38;5;28;01melse\u001B[39;00m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    272\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 273\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    275\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:575\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[0;32m    572\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m builder_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    573\u001B[0m   builder_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m--> 575\u001B[0m dbuilder \u001B[38;5;241m=\u001B[39m builder(name, data_dir\u001B[38;5;241m=\u001B[39mdata_dir, try_gcs\u001B[38;5;241m=\u001B[39mtry_gcs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs)\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m download:\n\u001B[0;32m    577\u001B[0m   download_and_prepare_kwargs \u001B[38;5;241m=\u001B[39m download_and_prepare_kwargs \u001B[38;5;129;01mor\u001B[39;00m {}\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\python39\\lib\\contextlib.py:79\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[1;32m---> 79\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:306\u001B[0m, in \u001B[0;36mbuilder.<locals>.decorator\u001B[1;34m(function, unused_none_instance, args, kwargs)\u001B[0m\n\u001B[0;32m    304\u001B[0m name \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m args \u001B[38;5;28;01melse\u001B[39;00m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 306\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    308\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:194\u001B[0m, in \u001B[0;36mbuilder\u001B[1;34m(name, try_gcs, **builder_kwargs)\u001B[0m\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbuilder_kwargs)  \u001B[38;5;66;03m# pytype: disable=not-instantiable\u001B[39;00m\n\u001B[0;32m    193\u001B[0m \u001B[38;5;66;03m# If neither the code nor the files are found, raise DatasetNotFoundError\u001B[39;00m\n\u001B[1;32m--> 194\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m not_found_error\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:175\u001B[0m, in \u001B[0;36mbuilder\u001B[1;34m(name, try_gcs, **builder_kwargs)\u001B[0m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;66;03m# First check whether we can find the corresponding dataset builder code\u001B[39;00m\n\u001B[0;32m    174\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 175\u001B[0m   \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mbuilder_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m registered\u001B[38;5;241m.\u001B[39mDatasetNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    177\u001B[0m   \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# Class not found\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\python39\\lib\\contextlib.py:79\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[1;34m(*args, **kwds)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[1;32m---> 79\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:114\u001B[0m, in \u001B[0;36mbuilder_cls\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    113\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mregistered\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimported_builder_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mds_name\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m typing\u001B[38;5;241m.\u001B[39mcast(Type[dataset_builder\u001B[38;5;241m.\u001B[39mDatasetBuilder], \u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\python39\\lib\\site-packages\\tensorflow_datasets\\core\\registered.py:279\u001B[0m, in \u001B[0;36mimported_builder_cls\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    276\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataset \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is an abstract class.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m _DATASET_REGISTRY:\n\u001B[1;32m--> 279\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m DatasetNotFoundError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDataset \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    281\u001B[0m builder_cls \u001B[38;5;241m=\u001B[39m _DATASET_REGISTRY[name]\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_builder_available(builder_cls):\n",
      "\u001B[1;31mDatasetNotFoundError\u001B[0m: Dataset imdb_reviews not found.\nAvailable datasets:\n\t- abstract_reasoning\n\t- accentdb\n\t- aeslc\n\t- aflw2k3d\n\t- ag_news_subset\n\t- ai2_arc\n\t- ai2_arc_with_ir\n\t- amazon_us_reviews\n\t- anli\n\t- answer_equivalence\n\t- arc\n\t- asqa\n\t- asset\n\t- assin2\n\t- bair_robot_pushing_small\n\t- bccd\n\t- beans\n\t- bee_dataset\n\t- beir\n\t- big_patent\n\t- bigearthnet\n\t- billsum\n\t- binarized_mnist\n\t- binary_alpha_digits\n\t- ble_wind_field\n\t- blimp\n\t- booksum\n\t- bool_q\n\t- bucc\n\t- caltech101\n\t- caltech_birds2010\n\t- caltech_birds2011\n\t- cardiotox\n\t- cars196\n\t- cassava\n\t- cats_vs_dogs\n\t- celeb_a\n\t- celeb_a_hq\n\t- chexpert\n\t- cifar10\n\t- cifar100\n\t- cifar100_n\n\t- cifar10_1\n\t- cifar10_corrupted\n\t- cifar10_n\n\t- citrus_leaves\n\t- cityscapes\n\t- clevr\n\t- clic\n\t- cmaterdb\n\t- coco\n\t- coco_captions\n\t- coil100\n\t- colorectal_histology\n\t- colorectal_histology_large\n\t- common_voice\n\t- controlled_noisy_web_labels\n\t- crema_d\n\t- curated_breast_imaging_ddsm\n\t- cycle_gan\n\t- deep_weeds\n\t- dementiabank\n\t- diabetic_retinopathy_detection\n\t- div2k\n\t- dmlab\n\t- domainnet\n\t- downsampled_imagenet\n\t- dsprites\n\t- dtd\n\t- duke_ultrasound\n\t- e2e_cleaned\n\t- efron_morris75\n\t- emnist\n\t- eurosat\n\t- fashion_mnist\n\t- flic\n\t- food101\n\t- fuss\n\t- geirhos_conflict_stimuli\n\t- groove\n\t- gtzan\n\t- gtzan_music_speech\n\t- horses_or_humans\n\t- i_naturalist2017\n\t- i_naturalist2018\n\t- i_naturalist2021\n\t- imagenet2012\n\t- imagenet2012_corrupted\n\t- imagenet2012_fewshot\n\t- imagenet2012_multilabel\n\t- imagenet2012_real\n\t- imagenet2012_subset\n\t- imagenet_a\n\t- imagenet_lt\n\t- imagenet_r\n\t- imagenet_resized\n\t- imagenet_sketch\n\t- imagenet_v2\n\t- imagenette\n\t- imagewang\n\t- kitti\n\t- kmnist\n\t- lfw\n\t- librispeech\n\t- libritts\n\t- ljspeech\n\t- lost_and_found\n\t- lsun\n\t- lvis\n\t- malaria\n\t- mnist\n\t- mnist_corrupted\n\t- nsynth\n\t- nyu_depth_v2\n\t- ogbg_molpcba\n\t- omniglot\n\t- open_images_challenge2019_detection\n\t- open_images_v4\n\t- oxford_flowers102\n\t- oxford_iiit_pet\n\t- pass\n\t- patch_camelyon\n\t- pet_finder\n\t- places365_small\n\t- placesfull\n\t- plant_leaves\n\t- plant_village\n\t- plantae_k\n\t- quickdraw_bitmap\n\t- resisc45\n\t- rock_paper_scissors\n\t- s3o4d\n\t- savee\n\t- scene_parse150\n\t- shapes3d\n\t- siscore\n\t- smallnorb\n\t- so2sat\n\t- speech_commands\n\t- spoken_digit\n\t- stanford_dogs\n\t- stanford_online_products\n\t- stl10\n\t- sun397\n\t- svhn_cropped\n\t- symmetric_solids\n\t- tedlium\n\t- tf_flowers\n\t- the300w_lp\n\t- uc_merced\n\t- universal_dependencies\n\t- user_libri_audio\n\t- vctk\n\t- visual_domain_decathlon\n\t- voc\n\t- voxceleb\n\t- voxforge\n\t- waymo_open_dataset\n\t- wider_face\n\t- xtreme_pos\n\t- xtreme_s\n\t- yes_no\n\nCheck that:\n    - if dataset was added recently, it may only be available\n      in `tfds-nightly`\n    - the dataset name is spelled correctly\n    - dataset class defines all base class abstract methods\n    - the module defining the dataset class is imported\n\nDid you mean: imdb_reviews -> amazon_us_reviews ?\n\nThe builder directory C:\\Users\\Eda AYDIN\\tensorflow_datasets\\imdb_reviews doesn't contain any versions.\nNo builder could be found in the directory: C:\\Users\\Eda AYDIN\\tensorflow_datasets for the builder: imdb_reviews.\nNo registered data_dirs were found in:\n\t- C:\\Users\\Eda AYDIN\\tensorflow_datasets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Download the plain text default config\n",
    "imdb_plaintext, info_plaintext = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "# Download the subword encoded pretokenized dataset\n",
    "imdb_subwords, info_subwords = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JggMZRCEcdlN"
   },
   "source": [
    "## Compare the two datasets\n",
    "\n",
    "As mentioned, the data types returned by the two datasets will be different. For the default, it will be strings as you also saw in Lab 1. Notice the description of the `text` key below and the sample sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3J7IAJMGH-VN"
   },
   "outputs": [],
   "source": [
    "# Print description of features\n",
    "info_plaintext.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTO45ghxc4js"
   },
   "outputs": [],
   "source": [
    "# Take 2 training examples and print the text feature\n",
    "for example in imdb_plaintext['train'].take(2):\n",
    "  print(example[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f87JvGD9dId5"
   },
   "source": [
    "For `subwords8k`, the dataset is already tokenized so the data type will be integers. Notice that the `text` features also include an `encoder` field and has a `vocab_size` of around 8k, hence the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wp_a7292mxk"
   },
   "outputs": [],
   "source": [
    "# Print description of features\n",
    "info_subwords.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ssDU_TddyLF"
   },
   "source": [
    "If you print the results, you will not see string sentences but a sequence of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35oQQIUG21cG"
   },
   "outputs": [],
   "source": [
    "# Take 2 training examples and print its contents\n",
    "for example in imdb_subwords['train'].take(2):\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWOrkYGug--B"
   },
   "source": [
    "You can get the `encoder` object included in the download and use it to decode the sequences above. You'll see that you will arrive at the same sentences provided in the `plain_text` config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4kNEGgEgfO6x"
   },
   "outputs": [],
   "source": [
    "# Get the encoder\n",
    "tokenizer_subwords = info_subwords.features['text'].encoder\n",
    "\n",
    "# Take 2 training examples and decode the text feature\n",
    "for example in imdb_subwords['train'].take(2):\n",
    "  print(tokenizer_subwords.decode(example[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20_XNWbXiwcE"
   },
   "source": [
    "*Note: The documentation for the encoder can be found [here](https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/SubwordTextEncoder) but don't worry if it's marked as deprecated. As mentioned, the objective of this exercise is just to show the characteristics of subword encoding.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKrbY2fjjFHM"
   },
   "source": [
    "## Subword Text Encoding\n",
    "\n",
    "From previous labs, the number of tokens in the sequence is the same as the number of words in the text (i.e. word tokenization). The following cells shows a review of this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6ly_yOIkM-K"
   },
   "outputs": [],
   "source": [
    "# Get the train set\n",
    "train_data = imdb_plaintext['train']\n",
    "\n",
    "# Initialize sentences list\n",
    "training_sentences = []\n",
    "\n",
    "# Loop over all training examples and save to the list\n",
    "for s,_ in train_data:\n",
    "  training_sentences.append(s.numpy().decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-N6Yd_TE3gZ5"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 10000\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer_plaintext = Tokenizer(num_words = 10000, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary for the training sentences\n",
    "tokenizer_plaintext.fit_on_texts(training_sentences)\n",
    "\n",
    "# Generate the training sequences\n",
    "sequences = tokenizer_plaintext.texts_to_sequences(training_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNUlDp76lf94"
   },
   "source": [
    "The cell above uses a `vocab_size` of 10000 but you'll find that it's easy to find OOV tokens when decoding using the lookup dictionary it created. See the result below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmsECyVr4OPE"
   },
   "outputs": [],
   "source": [
    "# Decode the first sequence using the Tokenizer class\n",
    "tokenizer_plaintext.sequences_to_texts(sequences[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0HQqkBmpujb"
   },
   "source": [
    "For binary classifiers, this might not have a big impact but you may have other applications that will benefit from avoiding OOV tokens when training the model (e.g. text generation). If you want the tokenizer above to not have OOVs, then the `vocab_size` will increase to more than 88k. This can slow down training and bloat the model size. The encoder also won't be robust when used on other datasets which may contain new words, thus resulting in OOVs again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u7m-Ds9lpUQc"
   },
   "outputs": [],
   "source": [
    "# Total number of words in the word index dictionary\n",
    "len(tokenizer_plaintext.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McxNKhHIsNvl"
   },
   "source": [
    "*Subword text encoding* gets around this problem by using parts of the word to compose whole words. This makes it more flexible when it encounters uncommon words. See how these subwords look like for this particular encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqyMSZbnwFBo"
   },
   "outputs": [],
   "source": [
    "# Print the subwords\n",
    "print(tokenizer_subwords.subwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaRA9LBUwfHM"
   },
   "source": [
    "If you use it on the previous plain text sentence, you'll see that it won't have any OOVs even if it has a smaller vocab size (only 8k compared to 10k above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn_eLaS5mR7H"
   },
   "outputs": [],
   "source": [
    "# Encode the first plaintext sentence using the subword text encoder\n",
    "tokenized_string = tokenizer_subwords.encode(training_sentences[0])\n",
    "print(tokenized_string)\n",
    "\n",
    "# Decode the sequence\n",
    "original_string = tokenizer_subwords.decode(tokenized_string)\n",
    "\n",
    "# Print the result\n",
    "print (original_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iL9O3hEqw4Bl"
   },
   "source": [
    "Subword encoding can even perform well on words that are not commonly found on movie reviews. See first the result when using the plain text tokenizer. As expected, it will show many OOVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHRj1J0j8ApE"
   },
   "outputs": [],
   "source": [
    "# Define sample sentence\n",
    "sample_string = 'TensorFlow, from basics to mastery'\n",
    "\n",
    "# Encode using the plain text tokenizer\n",
    "tokenized_string = tokenizer_plaintext.texts_to_sequences([sample_string])\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "# Decode and print the result\n",
    "original_string = tokenizer_plaintext.sequences_to_texts(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhQ-4O-uxdbJ"
   },
   "source": [
    "Then compare to the subword text encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPl2BXhYEHRP"
   },
   "outputs": [],
   "source": [
    "# Encode using the subword text encoder\n",
    "tokenized_string = tokenizer_subwords.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "# Decode and print the results\n",
    "original_string = tokenizer_subwords.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89sbfXjz0MSW"
   },
   "source": [
    "As you may notice, the sentence is correctly decoded. The downside is the token sequence is much longer. Instead of only 5 when using word-encoding, you ended up with 11 tokens instead. The mapping for this sentence is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3t7vvNLEZml"
   },
   "outputs": [],
   "source": [
    "# Show token to subword mapping:\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_subwords.decode([ts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ22ugch1TFy"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "You will now train your model using this pre-tokenized dataset. Since these are already saved as sequences, you can jump straight to making uniform sized arrays for the train and test sets. These are also saved as `tf.data.Dataset` type so you can use the [`padded_batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) method to create batches and pad the arrays into a uniform size for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVSTLBe_SOUr"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Get the train and test splits\n",
    "train_data, test_data = imdb_subwords['train'], imdb_subwords['test'], \n",
    "\n",
    "# Shuffle the training data\n",
    "train_dataset = train_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Batch and pad the datasets to the maximum length of the sequences\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
    "test_dataset = test_data.padded_batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCjHCG7s2sAR"
   },
   "source": [
    "Next, you will build the model. You can just use the architecture from the previous lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5NEpdhb8AxID"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define dimensionality of the embedding\n",
    "embedding_dim = 64\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(tokenizer_subwords.vocab_size, embedding_dim),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aOn2bAc3AUj"
   },
   "source": [
    "Similarly, you can use the same parameters for training. In Colab, it will take around 20 seconds per epoch (without an accelerator) and you will reach around 94% training accuracy and 88% validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkt8c5dNuUlT"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Set the training parameters\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Start training\n",
    "history = model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ygYaD6H3qGX"
   },
   "source": [
    "## Visualize the results\n",
    "\n",
    "You can use the cell below to plot the training results. See if you can improve it by tweaking the parameters such as the size of the embedding and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-_rMnm7WxQGT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot utility\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "# Plot the accuracy and results \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0TRE-Lb4C5b"
   },
   "source": [
    "## Wrap Up\n",
    "\n",
    "In this lab, you saw how subword text encoding can be a robust technique to avoid out-of-vocabulary tokens. It can decode uncommon words it hasn't seen before even with a relatively small vocab size. Consequently, it results in longer token sequences when compared to full word tokenization. Next week, you will look at other architectures that you can use when building your classifier. These will be recurrent neural networks and convolutional neural networks."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W2_Lab_3_imdb_subwords.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
